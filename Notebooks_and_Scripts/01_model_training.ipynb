{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "import gc\n",
    "import warnings\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "import math\n",
    "try:\n",
    "    import optuna.integration.lightgbm as lgb_optuna\n",
    "    OPTUNA_INTEGRATION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: `optuna-integration[lightgbm]` not found. Pruning callback disabled.\")\n",
    "    print(\"Install using: pip install optuna-integration[lightgbm]\")\n",
    "    OPTUNA_INTEGRATION_AVAILABLE = False\n",
    "\n",
    "# Import helper functions\n",
    "from utils import (\n",
    "    smape,\n",
    "    deg_to_sin,\n",
    "    deg_to_cos,\n",
    "    sincos_to_deg,\n",
    "    convert_units,\n",
    "    create_geo_clusters,\n",
    "    create_time_features,\n",
    "    create_lag_rolling_features_advanced,\n",
    "    select_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Suppress Warnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=optuna.exceptions.ExperimentalWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning) # Suppress fragmentation warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# --- Reproducibility & Control ---\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "N_SEEDS = 3\n",
    "OPTUNA_TRIALS = 100\n",
    "N_SPLITS_TSCV = 5\n",
    "\n",
    "TOP_N_FEATURES = 150\n",
    "\n",
    "# --- Data & Targets ---\n",
    "TARGET_COLS = [\"Avg_Temperature\", \"Radiation\", \"Rain_Amount\", \"Wind_Speed\", \"Wind_Direction\"]\n",
    "TARGETS_NORMAL = [\"Avg_Temperature\", \"Radiation\", \"Rain_Amount\", \"Wind_Speed\"]\n",
    "TARGET_WIND_DIR = \"Wind_Direction\"\n",
    "LOG_TRANSFORM_TARGETS = [\"Rain_Amount\", \"Radiation\", \"Wind_Speed\"]\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "N_CLUSTERS = 10\n",
    "LAG_ROLL_INPUT_COLS = [\"Avg_Temperature\", \"Avg_Feels_Like_Temperature\", \"Radiation\", \"Rain_Amount\", \"Rain_Duration\", \"Wind_Speed\", \"Temperature_Range\", \"Feels_Like_Temperature_Range\", \"Evapotranspiration\"]\n",
    "CATEGORICAL_FEATURES_BASE = ['kingdom', 'geo_cluster', 'month', 'dayofweek', 'year', 'quarter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_df_orig = pd.read_csv(\"../data/train.csv\"); \n",
    "    test_df_orig = pd.read_csv(\"../data/test.csv\"); \n",
    "    sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "except FileNotFoundError as e: print(f\"Error loading data: {e}. Exiting.\"); exit()\n",
    "\n",
    "if train_df_orig.empty or test_df_orig.empty: \n",
    "    print(\"Error: Input CSV is empty. Exiting.\"); exit()\n",
    "\n",
    "\n",
    "oof_preds_all_seeds = {} # Store OOF {seed: oof_df}\n",
    "test_preds_all_seeds = defaultdict(list) # Store test {target: [seed0_preds, seed1_preds,...]}\n",
    "\n",
    "for seed_run in range(N_SEEDS):\n",
    "    current_seed = GLOBAL_SEED + seed_run\n",
    "\n",
    "    print(f\"\\n{'='*25} Running Seed {seed_run+1}/{N_SEEDS} (Seed: {current_seed}) {'='*25}\")\n",
    "\n",
    "    np.random.seed(current_seed); optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    # --- Reload and Preprocess Data ---\n",
    "    train_df = train_df_orig.copy(); test_df = test_df_orig.copy()\n",
    "    train_df = convert_units(train_df)\n",
    "    train_df, test_df = create_geo_clusters(train_df, test_df, N_CLUSTERS, current_seed)\n",
    "    train_df = create_time_features(train_df)\n",
    "    test_df = create_time_features(test_df)\n",
    "\n",
    "    if train_df.empty or test_df.empty: \n",
    "        print(f\"DF empty after time features. Skipping seed {current_seed}.\"); \n",
    "        continue\n",
    "\n",
    "    # Wind Dir Handling\n",
    "    train_df[f'{TARGET_WIND_DIR}_sin'] = deg_to_sin(train_df[TARGET_WIND_DIR])\n",
    "    train_df[f'{TARGET_WIND_DIR}_cos'] = deg_to_cos(train_df[TARGET_WIND_DIR])\n",
    "    targets_with_sincos = TARGETS_NORMAL + [f'{TARGET_WIND_DIR}_sin', f'{TARGET_WIND_DIR}_cos']\n",
    "\n",
    "    # Advanced Features\n",
    "    lag_roll_inputs_this_run = [c for c in LAG_ROLL_INPUT_COLS if c in train_df.columns]\n",
    "    train_df = create_lag_rolling_features_advanced(train_df, lag_roll_inputs_this_run)\n",
    "    test_df = create_lag_rolling_features_advanced(test_df, lag_roll_inputs_this_run)\n",
    "\n",
    "    # --- Define Full Feature Set ---\n",
    "    print(\"\\nDefining full feature set...\")\n",
    "\n",
    "    all_cols = train_df.columns.tolist()\n",
    "    exclude_cols = TARGET_COLS + targets_with_sincos + ['ID', 'date', 'latitude', 'longitude']\n",
    "    initial_features = sorted([f for f in all_cols if f not in exclude_cols and f in test_df.columns])\n",
    "    categorical_features = [f for f in CATEGORICAL_FEATURES_BASE if f in initial_features]\n",
    "\n",
    "    print(f\"Initial feature set size: {len(initial_features)}\")\n",
    "\n",
    "    # Align Categoricals\n",
    "    print(\"Aligning final categorical dtypes...\")\n",
    "    for cat_col in categorical_features:\n",
    "        if cat_col in train_df.columns and cat_col in test_df.columns:\n",
    "\n",
    "            train_df[cat_col] = train_df[cat_col].astype('category'); test_df[cat_col] = test_df[cat_col].astype('category')\n",
    "            all_cats = pd.concat([train_df[cat_col].astype(str), test_df[cat_col].astype(str)]).unique()\n",
    "            train_df[cat_col] = pd.Categorical(train_df[cat_col].astype(str), categories=all_cats, ordered=False)\n",
    "            test_df[cat_col] = pd.Categorical(test_df[cat_col].astype(str), categories=all_cats, ordered=False)\n",
    "\n",
    "    # --- Data for Modeling ---\n",
    "    try: \n",
    "        X = train_df[initial_features].copy(); \n",
    "        Y = train_df[targets_with_sincos].copy(); \n",
    "        X_test_full = test_df[initial_features].copy()\n",
    "\n",
    "    except KeyError as e: \n",
    "        print(f\"Error preparing data arrays: Missing columns {e}. Skipping seed.\"); \n",
    "        continue\n",
    "\n",
    "    oof_df_seed = pd.DataFrame(index=X.index) # Initialize with the correct index\n",
    "    for target in targets_with_sincos:\n",
    "        oof_df_seed[target] = np.nan # Add columns one by one, filled with NaN\n",
    "\n",
    "\n",
    "    print(\"\\n--- Starting Model Training Loop (Per Target) ---\")\n",
    "    targets_processed_this_seed = []\n",
    "\n",
    "    for target in targets_with_sincos:\n",
    "        print(f\"\\n===== Processing Target: {target} (Seed: {current_seed}) =====\")\n",
    "\n",
    "        gc.collect(); y_target = Y[target].copy(); X_target = X.copy()\n",
    "\n",
    "        if y_target.isnull().any():\n",
    "            valid_indices = y_target.notna(); X_target = X_target.loc[valid_indices]; y_target = y_target.loc[valid_indices]\n",
    "\n",
    "            if X_target.empty: print(f\"  No data left. Skipping.\"); \n",
    "            continue\n",
    "\n",
    "        # Target Transformation\n",
    "        is_log_transformed = False; \n",
    "        inv_tf = lambda x: x\n",
    "\n",
    "        if target in LOG_TRANSFORM_TARGETS:\n",
    "\n",
    "            if (y_target <= 0).any(): \n",
    "                y_target_transformed = np.log1p(y_target + 1e-6)\n",
    "\n",
    "            else: y_target_transformed = np.log1p(y_target)\n",
    "\n",
    "            inv_tf = np.expm1; is_log_transformed = True; print(\"  Log1p Transform Applied.\")\n",
    "\n",
    "        else: y_target_transformed = y_target\n",
    "\n",
    "        # Feature Selection\n",
    "        selected_features = select_features(X_target, y_target_transformed, initial_features, TOP_N_FEATURES, categorical_features, current_seed)\n",
    "\n",
    "        if not selected_features: \n",
    "            print(\"  Error: No features selected. Skipping target.\"); \n",
    "            continue\n",
    "\n",
    "        X_train_fs = X_target[selected_features]; X_test_fs = X_test_full[selected_features].copy()\n",
    "\n",
    "        # Imputation & Scaling\n",
    "        print(f\"  Processing selected features ({len(selected_features)})...\")\n",
    "\n",
    "        numerical_features_selected = X_train_fs.select_dtypes(include=np.number).columns.tolist()\n",
    "        imputer, scaler = None, None\n",
    "\n",
    "        if numerical_features_selected:\n",
    "\n",
    "            if X_train_fs[numerical_features_selected].isnull().any().any():\n",
    "\n",
    "                print(\"    Imputing...\"); \n",
    "                imputer = SimpleImputer(strategy='median'); \n",
    "                X_train_fs[numerical_features_selected] = imputer.fit_transform(X_train_fs[numerical_features_selected]); \n",
    "                X_test_fs[numerical_features_selected] = imputer.transform(X_test_fs[numerical_features_selected])\n",
    "\n",
    "            print(\"    Scaling...\"); \n",
    "            scaler = StandardScaler(); \n",
    "            X_train_fs[numerical_features_selected] = scaler.fit_transform(X_train_fs[numerical_features_selected]); \n",
    "            X_test_fs[numerical_features_selected] = scaler.transform(X_test_fs[numerical_features_selected])\n",
    "\n",
    "        cats_selected = [c for c in categorical_features if c in selected_features]\n",
    "        for cat_col in cats_selected:\n",
    "             all_cats = pd.concat([X_train_fs[cat_col].astype(str), X_test_fs[cat_col].astype(str)]).unique()\n",
    "             X_train_fs[cat_col] = pd.Categorical(X_train_fs[cat_col].astype(str), categories=all_cats, ordered=False)\n",
    "             X_test_fs[cat_col] = pd.Categorical(X_test_fs[cat_col].astype(str), categories=all_cats, ordered=False)\n",
    "\n",
    "        if X_train_fs.isnull().any().any() or X_test_fs.isnull().any().any(): \n",
    "            print(\"CRITICAL WARNING: NaNs detected AFTER imputation/scaling. Skipping target.\"); \n",
    "            continue\n",
    "\n",
    "        # Optuna Tuning\n",
    "        print(f\"  Starting Optuna tuning ({OPTUNA_TRIALS} trials, Data: {X_train_fs.shape})...\")\n",
    "\n",
    "        def objective(trial):\n",
    "            min_samples_needed = N_SPLITS_TSCV + 1\n",
    "\n",
    "            if len(X_train_fs) < min_samples_needed: \n",
    "                raise optuna.exceptions.TrialPruned(f\"Samples ({len(X_train_fs)}) < N_SPLITS+1\")\n",
    "            \n",
    "            params = { 'objective': 'regression_l1', 'metric': 'mae', 'verbosity': -1, 'n_jobs': -1, 'seed': \n",
    "                      current_seed, 'boosting_type': 'gbdt', 'n_estimators': \n",
    "                      trial.suggest_int('n_estimators', 500, 4000, step=100), 'learning_rate': \n",
    "                      trial.suggest_float('learning_rate', 0.005, 0.05), 'num_leaves': \n",
    "                      trial.suggest_int('num_leaves', 20, 150), 'max_depth': \n",
    "                      trial.suggest_int('max_depth', 5, 16), 'subsample': \n",
    "                      trial.suggest_float('subsample', 0.5, 1.0, step=0.05), 'colsample_bytree': \n",
    "                      trial.suggest_float('colsample_bytree', 0.5, 1.0, step=0.05), 'reg_alpha': \n",
    "                      trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True), 'reg_lambda': \n",
    "                      trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True), 'subsample_freq': \n",
    "                      trial.suggest_int('subsample_freq', 0, 7), 'min_child_samples': \n",
    "                      trial.suggest_int('min_child_samples', 5, 50) }\n",
    "            \n",
    "            tscv = TimeSeriesSplit(n_splits=N_SPLITS_TSCV); scores = []\n",
    "            lgbm_cats_obj = [c for c in categorical_features if c in X_train_fs.columns] or 'auto'\n",
    "            oof_preds_fold = np.full(len(X_train_fs), np.nan)\n",
    "\n",
    "            try:\n",
    "                for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_fs)):\n",
    "\n",
    "                    if len(val_idx) == 0: \n",
    "                        continue\n",
    "\n",
    "                    X_tr, X_val = X_train_fs.iloc[train_idx], X_train_fs.iloc[val_idx]; \n",
    "                    y_tr, y_val = y_target_transformed.iloc[train_idx], y_target_transformed.iloc[val_idx]; \n",
    "                    y_val_orig = y_target.iloc[val_idx]\n",
    "\n",
    "                    model = LGBMRegressor(**params)\n",
    "                    callbacks_list = [lgbm.early_stopping(100, verbose=False)]\n",
    "\n",
    "                    if OPTUNA_INTEGRATION_AVAILABLE: \n",
    "                        callbacks_list.insert(0, lgb_optuna.LightGBMPruningCallback(trial, 'l1'))\n",
    "\n",
    "                    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='mae', callbacks=callbacks_list, categorical_feature=lgbm_cats_obj)\n",
    "                    preds_val_tf = model.predict(X_val); preds_val_orig = inv_tf(preds_val_tf)\n",
    "                    oof_preds_fold[val_idx] = preds_val_orig # Store OOF\n",
    "\n",
    "                    # Clip for scoring\n",
    "                    if target in LOG_TRANSFORM_TARGETS or target == \"Radiation\": \n",
    "                        preds_val_orig = np.clip(preds_val_orig, 0, None)\n",
    "\n",
    "                    if target.endswith('_sin'): \n",
    "                        preds_val_orig = np.clip(preds_val_orig, -1, 1)\n",
    "\n",
    "                    if target.endswith('_cos'): \n",
    "                        preds_val_orig = np.clip(preds_val_orig, -1, 1)\n",
    "\n",
    "                    if np.isnan(preds_val_orig).any() or np.isnan(y_val_orig).any(): \n",
    "                        continue\n",
    "\n",
    "                    scores.append(smape(y_val_orig, preds_val_orig))\n",
    "\n",
    "            except optuna.exceptions.TrialPruned as e: raise\n",
    "            \n",
    "            except Exception as e: print(f\" Objective Error: {e}\"); traceback.print_exc(); raise optuna.exceptions.TrialPruned(f\"Obj Err: {e}\")\n",
    "\n",
    "            if not scores: raise optuna.exceptions.TrialPruned(\"No valid scores.\")\n",
    "\n",
    "            trial.set_user_attr(\"oof_predictions\", oof_preds_fold) # Store full OOF array for trial\n",
    "            return np.mean(scores) # Return sMAPE\n",
    "\n",
    "        pruner = optuna.pruners.MedianPruner(n_startup_trials=15, n_warmup_steps=30, interval_steps=10)\n",
    "        study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "\n",
    "        try: \n",
    "            study.optimize(objective, n_trials=OPTUNA_TRIALS, show_progress_bar=True, catch=(Exception,))\n",
    "            \n",
    "        except Exception as e: print(f\" Optuna optimize failed: {e}\"); continue\n",
    "        \n",
    "        if study.best_trial is None: \n",
    "            print(f\" No best trial found. Skipping.\"); \n",
    "            continue\n",
    "\n",
    "        print(f\"  Best CV sMAPE: {study.best_value:.4f}\")\n",
    "\n",
    "        # Store best OOF\n",
    "        best_oof = study.best_trial.user_attrs.get(\"oof_predictions\")\n",
    "\n",
    "        if best_oof is not None:\n",
    "\n",
    "            valid_oof_indices = ~np.isnan(best_oof)\n",
    "            original_indices_for_oof = X_train_fs.index[valid_oof_indices]\n",
    "            valid_oof_values = best_oof[valid_oof_indices]\n",
    "\n",
    "            oof_df_seed.loc[original_indices_for_oof, target] = valid_oof_values\n",
    "\n",
    "        else: \n",
    "            print(\"  Warning: Could not retrieve OOF predictions from best trial.\")\n",
    "\n",
    "        # Train Final Model\n",
    "        print(f\"  Training final model...\")\n",
    "\n",
    "        final_params = study.best_trial.params; \n",
    "        final_params.update({'objective': 'regression_l1', 'metric': 'mae', 'verbosity': -1, 'n_jobs': -1, 'seed': current_seed, 'boosting_type': 'gbdt'})\n",
    "        final_model = LGBMRegressor(**final_params); lgbm_cats_final = [c for c in categorical_features if c in X_train_fs.columns] or 'auto'\n",
    "\n",
    "        try: \n",
    "            final_model.fit(X_train_fs, y_target_transformed, categorical_feature=lgbm_cats_final)\n",
    "        except Exception as e: \n",
    "            print(f\"  Error training final model: {e}\"); \n",
    "            continue\n",
    "\n",
    "        # Predict & Store Test\n",
    "        try:\n",
    "            print(f\"  Predicting test data ({X_test_fs.shape})...\")\n",
    "\n",
    "            test_preds_tf = final_model.predict(X_test_fs)\n",
    "            test_preds_orig = inv_tf(test_preds_tf)\n",
    "\n",
    "            if len(test_preds_orig) == len(test_df): \n",
    "                test_preds_all_seeds[target].append(test_preds_orig); \n",
    "                targets_processed_this_seed.append(target)\n",
    "\n",
    "            else: \n",
    "                print(f\"  Error: Pred length ({len(test_preds_orig)}) != Test length ({len(test_df)}).\")\n",
    "\n",
    "        except Exception as e: print(f\"  Error predicting: {e}\")\n",
    "\n",
    "        del final_model, study, X_target, y_target, X_train_fs, X_test_fs, y_target_transformed; gc.collect()\n",
    "\n",
    "    oof_preds_all_seeds[current_seed] = oof_df_seed # Store this seed's OOF\n",
    "\n",
    "    print(f\"--- Finished Seed {seed_run+1}/{N_SEEDS} (Processed: {len(targets_processed_this_seed)} targets) ---\")\n",
    "\n",
    "    del train_df, test_df, X, Y, X_test_full; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensemble Test Predictions ---\n",
    "final_test_preds_agg = {}\n",
    "\n",
    "if not test_preds_all_seeds: print(\"CRITICAL ERROR: No predictions generated. Cannot create submission.\"); exit()\n",
    "\n",
    "common_targets = set(test_preds_all_seeds.keys())\n",
    "\n",
    "print(f\"Targets with predictions: {common_targets}\")\n",
    "\n",
    "expected_len = len(test_df_orig) # Use original test length\n",
    "\n",
    "for target in common_targets:\n",
    "    valid_preds = [p for p in test_preds_all_seeds[target] if p is not None and isinstance(p, np.ndarray) and len(p) == expected_len]\n",
    "    if not valid_preds: print(f\"Warning: No valid predictions for {target}. Filling with 0.\"); final_test_preds_agg[target] = np.zeros(expected_len); continue\n",
    "    print(f\"  Averaging {len(valid_preds)} predictions for {target}...\")\n",
    "    final_test_preds_agg[target] = np.mean(np.array(valid_preds), axis=0)\n",
    "\n",
    "# --- Convert Wind Dir ---\n",
    "wind_sin_key = f'{TARGET_WIND_DIR}_sin'; wind_cos_key = f'{TARGET_WIND_DIR}_cos'\n",
    "if wind_sin_key in final_test_preds_agg and wind_cos_key in final_test_preds_agg:\n",
    "    print(\"Converting Wind Dir sin/cos to degrees...\")\n",
    "    wind_sin = np.clip(final_test_preds_agg[wind_sin_key], -1.0, 1.0); wind_cos = np.clip(final_test_preds_agg[wind_cos_key], -1.0, 1.0)\n",
    "    final_test_preds_agg[TARGET_WIND_DIR] = sincos_to_deg(wind_sin, wind_cos)\n",
    "    if wind_sin_key in final_test_preds_agg: del final_test_preds_agg[wind_sin_key]\n",
    "    if wind_cos_key in final_test_preds_agg: del final_test_preds_agg[wind_cos_key]\n",
    "else: print(f\"Warning: Sin/Cos preds for {TARGET_WIND_DIR} not found. Filling Dir with 0.\"); final_test_preds_agg[TARGET_WIND_DIR] = 0\n",
    "\n",
    "# --- Post-Processing ---\n",
    "print(\"Post-processing final predictions...\")\n",
    "for target in TARGET_COLS: # Use original target list\n",
    "    if target in final_test_preds_agg:\n",
    "        preds = final_test_preds_agg[target]\n",
    "        if target in [\"Rain_Amount\", \"Radiation\", \"Wind_Speed\"]: preds = np.clip(preds, 0, None); preds[preds < 1e-4] = 0\n",
    "        if target == TARGET_WIND_DIR: preds = np.clip(preds, 0, 360)\n",
    "        if target == \"Avg_Temperature\": preds = np.clip(preds, -50, 60)\n",
    "        final_test_preds_agg[target] = preds\n",
    "    else: print(f\"Warning: {target} missing in final dict. Filling with 0.\"); final_test_preds_agg[target] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Submission ---\n",
    "print(\"\\nCreating final submission file...\")\n",
    "try:\n",
    "    submission_df = pd.DataFrame({'ID': test_df_orig['ID']})\n",
    "\n",
    "    pred_df = pd.DataFrame({k: (v if isinstance(v, np.ndarray) and len(v) == expected_len else np.zeros(expected_len))\n",
    "                           for k, v in final_test_preds_agg.items()})\n",
    "\n",
    "    for col in TARGET_COLS:\n",
    "        if col in pred_df.columns: submission_df[col] = pred_df[col].values\n",
    "        else: print(f\"Warning: Target '{col}' final prediction missing. Filling with 0.\"); submission_df[col] = 0.0\n",
    "\n",
    "    # Merge with sample submission structure for final format check\n",
    "    # NOTE: sample_submission is defined globally at the start\n",
    "    final_submission = sample_submission[['ID']].merge(submission_df, on='ID', how='left')\n",
    "    if final_submission.isnull().any().any(): print(\"Warning: NaNs after final merge. Filling with 0.\"); final_submission = final_submission.fillna(0)\n",
    "    final_submission = final_submission[sample_submission.columns] # Ensure column order\n",
    "\n",
    "    submission_filename = \"submission_advanced_v1_fixed_oof.csv\" # Updated filename\n",
    "    final_submission.to_csv(submission_filename, index=False)\n",
    "    print(f\"Submission file created: {submission_filename}\")\n",
    "    print(final_submission.head())\n",
    "except NameError as e:\n",
    "     print(f\"Error creating submission: Variable not defined? {e}\") # Catch if sample_submission wasn't loaded\n",
    "except Exception as e: print(f\"Error creating/saving submission file: {e}\"); traceback.print_exc()\n",
    "\n",
    "\n",
    "print(\"\\nPipeline Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
